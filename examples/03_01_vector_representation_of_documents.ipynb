{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiAnKtlKK-9H"
   },
   "source": [
    "# Pridobitev vektorskih predstavitev besedil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6PJCzZjlI9f"
   },
   "source": [
    "V tem zvezku predstavimo, kako lahko pridobimo vektorske predstavitve (vložitve) besed in dokumentov za analizo besedil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSZ5JxEKKvLZ"
   },
   "source": [
    "Za začetek si preko API-ja pridobimo besedila zadnjih 100 predlogov vladi, ki vsebujejo vsaj 50 znakov (s tem pogojem poskrbimo, da po predprocesiranju ne dobimo praznega seznama pojavnic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxGoFv_5rjgP",
    "outputId": "762835fe-4a2f-432c-9ec1-5c5d7c8d4cad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Število predlogov vladi: 99\n"
     ]
    }
   ],
   "source": [
    "from textsemantics.server_api import ServerAPI\n",
    "\n",
    "api = ServerAPI()\n",
    "metadata = api.get_metadata('predlogi-vladi', sample_size=100, sampling_strategy='latest')\n",
    "\n",
    "texts = api.get_texts(urls=metadata['text'])\n",
    "texts = [text for text in texts if len(text) > 50]\n",
    "print(f'Število predlogov vladi: {len(texts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIyjtMDbK4NN"
   },
   "source": [
    "Dobili smo 99 dokumentov. Zdaj lahko dokumente predprocesiramo tako, da iz njih odstranimo končnice, jih pretvorimo v seznam pojavnic, odstranimo prazne besede in lematiziramo preostale pojavnice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "WmHmUeH6sa6M",
    "outputId": "c94dcf68-ce73-4433-d80f-0a1b5b431492"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Prvih 10 pojavnic v prvem dokumentu\n",
       "- meniti\n",
       "- slovenija\n",
       "- obdavčiti\n",
       "- gospodinjstvo\n",
       "- podjetje\n",
       "- obdavčiti\n",
       "- verski\n",
       "- institucija\n",
       "- obdavčiti\n",
       "- verski\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from lemmagen.lemmatizer import Lemmatizer\n",
    "from lemmagen import DICTIONARY_SLOVENE\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def preprocess(corpus):\n",
    "    stop_words = set(stopwords.words('slovene'))\n",
    "    tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "    lemmatizer = Lemmatizer(dictionary=DICTIONARY_SLOVENE)\n",
    "    \n",
    "    preprocessed = list()\n",
    "    for text in corpus:\n",
    "        text = text.translate(text.maketrans('', '', string.punctuation))\n",
    "        tokens = tokenizer.tokenize(text.lower())\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words \n",
    "                  and len(token) > 2 and not token.isnumeric()]\n",
    "        preprocessed.append(tokens)\n",
    "        \n",
    "    return preprocessed\n",
    "\n",
    "tokens_list = preprocess(texts)\n",
    "\n",
    "md_string = '### Prvih 10 pojavnic v prvem dokumentu\\n'\n",
    "for tok in tokens_list[0][:10]:\n",
    "    md_string += f\"- {tok}\\n\"\n",
    "display(Markdown(md_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zVj8fFLLCMs"
   },
   "source": [
    "Sedaj lahko vsak dokument predstavimo kot vektor. Vektorje bomo dobili z uporabo vreče besed, kjer vsak atribut predstavlja eno besedo v slovarju, vsaka vrstica pa en dokument. Tabela predstavi število pojavitev posamezne besede za posamezen dokument. Tabelo lahko prilagodimo tako, da upoštevamo pogostost besed - manj pogoste, a pomembne besede bodo imele višjo vrednost kot take, ki so vseprisotne. Poleg tega bomo dokumente predstavili z uporabo modela fastText, ki temelji na nevronskih mrežah in je prednaučen na velikem korpusu dokumentov. V osnovi je fastText učen, da besede predstavi z nizkodimenzionalnimi vektorji, vendar lahko vektorje dokumentov dobimo s povprečenjem vektorjev besed, ki se v dokumentu nahajajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3ZvZA5GLJGM",
    "outputId": "e0c770fc-74dc-44ce-88e9-3b8b4157d9c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrika fastText: 99 vrstic, 300 stolpcev\n",
      "Matrika vreče besed (tf-idf): 99 vrstic, 2270 stolpcev\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def vectorize(tokens_list, emb_type='fasttext'):\n",
    "    joined_texts = [' '.join(tokens) for tokens in tokens_list]\n",
    "\n",
    "    if emb_type=='fasttext':\n",
    "        embedder = DocumentPoolEmbeddings([WordEmbeddings('sl')],\n",
    "                                          pooling='mean')\n",
    "        X = list()\n",
    "        for i, doc in enumerate(joined_texts):\n",
    "            sent = Sentence(doc)\n",
    "            embedder.embed(sent)\n",
    "            X.append(sent.embedding.cpu().detach().numpy())\n",
    "        return np.array(X)\n",
    "    elif emb_type == 'tfidf':\n",
    "        return TfidfVectorizer().fit_transform(joined_texts)   \n",
    "    return None\n",
    "\n",
    "ft = vectorize(tokens_list, emb_type='fasttext')\n",
    "tfidf = vectorize(tokens_list, emb_type='tfidf')\n",
    "\n",
    "print(f'Matrika fastText: {ft.shape[0]} vrstic, {ft.shape[1]} stolpcev')\n",
    "print(f'Matrika vreče besed (tf-idf): {tfidf.shape[0]} vrstic, {tfidf.shape[1]} stolpcev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LlKfL5vmWTl"
   },
   "source": [
    "Za vložitve tf-idf smo dobili 99 x 2270 matriko, za vložitve fastText pa 99 x 300 matriko. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dvi3h_JLwHe"
   },
   "source": [
    "Dobljene vektorje si shranimo v datoteko, da bi jih lahko uporabljali v nadaljnjih primerih."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Qg6OS-RDLvXZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "def save_data(ft, tfidf):\n",
    "    word_embs = list()\n",
    "    embedder = WordEmbeddings('sl')\n",
    "\n",
    "    for word in ['šola', 'počitnice', 'semafor', 'tehnologija']:\n",
    "        sent = Sentence(word)\n",
    "        embedder.embed(sent)\n",
    "        vec = sent.tokens[0].embedding.cpu().detach().numpy()\n",
    "        word_embs.append(vec)\n",
    "    word_embs = np.array(word_embs)\n",
    "\n",
    "    try:\n",
    "        os.mkdir('data')\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    np.save('data/ft.npy', ft)\n",
    "    save_npz('data/tfidf.npz', tfidf)\n",
    "    np.save('data/words.npy', word_embs)\n",
    "\n",
    "save_data(ft, tfidf)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02-02-vector-representation-of-documents.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
