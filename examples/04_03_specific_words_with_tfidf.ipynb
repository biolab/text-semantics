{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGwuELZZUfED"
   },
   "source": [
    "# Iskanje besed, specifičnih za dokumente z uporabo transformacije TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmEW2fr2Up_2"
   },
   "source": [
    "Tokrat bomo poskusili specifične besede v dokumentu določiti z uporabo transformacije TF-IDF, ki uteži besede glede na njihovo pogostost v besedilu. Besede, ki močno zaznamujejo manjšo množico dokumentov, bodo tako imele večjo težo kot take, ki so vseprisotne v korpusu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeLVMjMLWVhR"
   },
   "source": [
    "Iz seznama predlogov vladi izberemo zadnjih 100 dokumentov, jih predobledamo na enak način kot v prejšnjih primerih in za vsak dokument izračunamo najbolj specifične besede. Uporabili bomo utežitev TF-IDF in poiskali nekaj najvišje uteženih besed v množici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E7oBUANqIGzm"
   },
   "outputs": [],
   "source": [
    "from textsemantics.server_api import ServerAPI\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from lemmagen.lemmatizer import Lemmatizer\n",
    "from lemmagen import DICTIONARY_SLOVENE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def preprocess(corpus):\n",
    "    stop_words = set(stopwords.words('slovene'))\n",
    "    tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "    lemmatizer = Lemmatizer(dictionary=DICTIONARY_SLOVENE)\n",
    "    \n",
    "    preprocessed = list()\n",
    "    for text in corpus:\n",
    "        text = text.translate(text.maketrans('', '', string.punctuation))\n",
    "        tokens = tokenizer.tokenize(text.lower())\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words \n",
    "                  and len(token) > 2 and not token.isnumeric()]\n",
    "        preprocessed.append(tokens)\n",
    "        \n",
    "    return preprocessed\n",
    "\n",
    "api = ServerAPI()\n",
    "datasets = api.list_datasets()\n",
    "metadata = api.get_metadata(datasets[2][0], sample_size=100, sampling_strategy='latest')\n",
    "\n",
    "texts = api.get_texts(urls=metadata['text'])\n",
    "texts = [text for text in texts if len(text) > 50]\n",
    "\n",
    "tokens_list = preprocess(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dLcquNeRIl0a"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "joined_texts = [' '.join(tokens) for tokens in tokens_list]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(joined_texts)\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tfidf_words(document_id, tfidf, feature_names, n_words=5):\n",
    "    n_words = n_words if n_words and n_words <= len(feature_names) else len(feature_names)\n",
    "    feature_index = tfidf[document_id,:].nonzero()[1]\n",
    "    return sorted(((feature_names[i], tfidf[document_id, i]) for i in feature_index), key=lambda tup: tup[1], reverse=True)[:n_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nato izberemo, koliko najvišje rangiranih besed nas zanima, ter kateri dokument bomo opazovali. Izbrali smo najbolj zanimivih pet be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_words = 5\n",
    "doc_id = 13\n",
    "top_words = find_tfidf_words(doc_id, X, feature_names, n_words=num_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-BQewUSqXzAi"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def display_document_and_specific_words(doc_ind, top_words): \n",
    "    s = f\"## {metadata.iloc[doc_ind]['title']}\\n {texts[doc_ind]}\\n\"\n",
    "    s += f\"\\n### {num_of_words} najvišje uteženih besed z metodo TF-IDF:\\n\"\n",
    "    s += \"|Beseda|Utež|\\n\"\n",
    "    s += \"|---|---|\\n\"\n",
    "    for word, value in top_words:\n",
    "        s += f'|{word:<15}|{value:.2f}|\\n'\n",
    "\n",
    "    display(Markdown(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwJ0sr9BaOJN"
   },
   "source": [
    "Poglejmo en dokument in 5 najbolj specifičnih besed, ki jih najde utežitev TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "NZUK3dHBX_BM",
    "outputId": "9504042e-8fd1-4f5c-cf93-d46e20431699",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Videvanja z partnerjem z tujine med epidemijo.\n",
       " Predlagam, da se kot izjema za prehajanje regije, občine ali državne meje, uvede tudi vzdrževanje stikov z partnerji iz tujine. Sam, kot tudi drugi smo namreč zaradi tega prizadeti, saj nas veliko živi ob meji s Hrvaško na primer, 15 min vožnje stran eden od drugega, a zaradi restriktivnih srečanje ni možno, čez Kolpo si pa lahko mahamo. Je bil 1. Val dovolj, da se nisem mogel videti s punco, katere tudi skoraj nisem prepoznal, po 2 mesecih razdvojenosti. Lp\n",
       "\n",
       "### 5 najvišje uteženih besed z metodo TF-IDF:\n",
       "|Beseda|Utež|\n",
       "|---|---|\n",
       "|meja           |0.32|\n",
       "|razdvojenost   |0.20|\n",
       "|punca          |0.20|\n",
       "|mahati         |0.20|\n",
       "|kolpa          |0.20|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_document_and_specific_words(doc_id, top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02-05-specific-words-with-embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
